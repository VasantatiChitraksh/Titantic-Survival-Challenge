# -*- coding: utf-8 -*-
"""Getting Started With Titanic-1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/getting-started-with-titanic-1-1dc7fb6c-0643-44af-9af0-b452214d3f80.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240615/auto/storage/goog4_request%26X-Goog-Date%3D20240615T125709Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4bbd8fbee2aee7507ef5dbc05c4999f18040a13737f5e37eb16041e80264a4d97ccc2b45518535f0c4bef3ec3fb4a31db2acbb8a7f3b2a186bd511c12de80614ead62f839fc9b6f2742e776341625575139887a0f28b09714b69a9b3178a99885586ce06564d30afb06d89535f58829b0c34e04867b610f622c29ddd19c986fc787221218c0448efd57d4a02e7c8157e0d7eecd2aae6f8f6fd2946fb75fc2b38b811731f9ef942f274d4980a098269e1fdd2504caae137a2a0a666c86447f6efb2649b96fa47ccd305edfaf515960530862050b39552b774b276cdde70588b12115f15aabd2db7ef86202c86f752edcd89503b100bf618b8bde8b0619d095ef3
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'titanic:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F3136%2F26502%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240615%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240615T125709Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7d7a1bf77803bc31b15e06006c146e56843314bc76803b05abb4dc43401915b366485274689eb044baeb348e2e77b0c8a02f880fbc8e0c75034cb7cf86d950e3631e5ca5cb7d746c82583b715c16bab8fc75b4a0be7512871e11befb2f9f5b5aa8fd813785df197ce87bd38ffd302f0c82acc8be065c4a14d8895478d85c6001e8c154fd4ab5e9fe51f2e779cc64da4b367de9945fa6cf512a0557af964d643997ad42aad0fdb47e39128e8640ecee1e4e4b461fe464c5b904a95c1d9f96f293b82c677d3c13718a32921ade76eca7bfcb8abeb2a73aa3e609eca124a81ad90df040e44e4070e86c71dfff7dd62cfc7112b64dcf309fd284134b3c92c65151b3'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""import Seaborn as sn"""

import seaborn as sn

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

train_data = pd.read_csv("/kaggle/input/titanic/train.csv")
train_data.head()

test_data = pd.read_csv("/kaggle/input/titanic/test.csv")
test_data.head()

men = train_data.loc[train_data.Sex == 'male']["Survived"]
rate_men = sum(men)/len(men)

print("% of men who survived:", rate_men)

women = train_data.loc[train_data.Sex == 'female']["Survived"]
rate_women = sum(women)/len(women)

print("% of women who survived:", rate_women)

# # train_data.columns
# # test_data.columns
# train_data.isnull().sum()
train_data.head(150)

test_data.isnull().sum()

from sklearn.preprocessing import OneHotEncoder , LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer
from sklearn.model_selection import train_test_split
# Define imputer for numerical data (Age) with strategy (e.g., 'mean')
imputer = SimpleImputer(strategy="mean")

# Impute missing values in Age column for both train and test data
train_data[['Age','Fare']] = imputer.fit_transform(train_data[['Age','Fare']])
test_data[['Age','Fare']] = imputer.transform(test_data[['Age','Fare']])  # Use fitted imputer on test data

# Label Encoder for categorical data (Sex)
label_encoder = LabelEncoder()

# Encode Sex column for both train and test data
train_data['Sex'] = label_encoder.fit_transform(train_data['Sex'])
test_data['Sex'] = label_encoder.transform(test_data['Sex'])  # Use fitted encoder on test data

features = ['Pclass','Sex','Age','Fare','SibSp','Parch']
y = train_data['Survived']
X = train_data[features]
X_test = test_data[features]
input_shape = [X.shape[1]]

X.isnull().sum()

from tensorflow.keras.models import Sequential
from tensorflow.keras import layers
from tensorflow import keras

model = Sequential([
    layers.Dense(units=512,activation='relu',input_shape=input_shape),
    layers.Dense(units=256,activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.3),
    layers.Dense(units=1,activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss = 'binary_crossentropy',
    metrics=['binary_accuracy'])

early_stopping = keras.callbacks.EarlyStopping(
    patience=20,
    min_delta=0.001,
    restore_best_weights=True,
)

model.fit(
    X, y,
    batch_size=64,
    epochs=200,
    callbacks=[early_stopping],
    verbose = 0
)

predictions = model.predict(X_test)
survival_threshold = 0.69  # You can adjust this threshold
temp = pd.DataFrame({
    'PassengerId': test_data.PassengerId,
    'Survived': predictions[:, 0]  # Apply threshold for classification
})
output = pd.DataFrame({
    'PassengerId': test_data.PassengerId,
    'Survived': predictions[:, 0] > survival_threshold  # Apply threshold for classification
})
output.head()

len(temp[temp['Survived']>0.69])

temp.mean()

output['Survived']=output['Survived'].astype(int)

output.head(20)

output.to_csv('submission2.csv', index=False)
print("Your submission was successfully saved!")